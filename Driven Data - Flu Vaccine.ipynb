{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import (mean, std)\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import (cross_val_score, RepeatedStratifiedKFold)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import (StackingClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, \n",
    "                             AdaBoostClassifier)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from numpy import where\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_set_features.csv')\n",
    "test['label'] = 'test'\n",
    "train = pd.read_csv('training_set_features.csv')\n",
    "train['label'] = 'train'\n",
    "labels = pd.read_csv('training_set_labels.csv')\n",
    "h1n1 = labels['h1n1_vaccine']\n",
    "seasonal = labels['seasonal_vaccine']\n",
    "combine = pd.concat([train, test], axis = 0)\n",
    "ID = test['respondent_id']\n",
    "combine = combine.drop(['respondent_id'], axis = 1)\n",
    "full_train = pd.concat([labels, train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_cols = combine.columns\n",
    "labels_cols = labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check column names\n",
    "print(combine_cols)\n",
    "print(labels_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the dataframes\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(labels.shape)\n",
    "print(combine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the response variables\n",
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26707 total rows\n",
    "#21.2% got h1n1_vaccine\n",
    "#46.6% got seasonal vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (combine.isnull().sum()/len(combine))\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\n",
    "missing_data = pd.DataFrame({'Missing Ratio':all_data_na})\n",
    "missing_data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like employment occupation and employment industry is code for not in work force where blank.\n",
    "combine['employment_industry'] = combine['employment_industry'].fillna('abcde')\n",
    "combine['employment_occupation'] = combine['employment_occupation'].fillna('fghij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "combine[[\"msa\", \"city\"]] = (  # Create two new features\n",
    "    combine[\"census_msa\"]           # from the Policy feature\n",
    "    .str                         # through the string accessor\n",
    "    .split(\",\", expand=True)     # by splitting on \" \"\n",
    "                                 # and expanding the result into separate columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['race_and_sex'] = combine['race'] + \"_\" + combine['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = combine.select_dtypes(include = 'object').columns\n",
    "print(label_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process columns, apply LabelEncoder to categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for c in label_enc:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(combine[c].values)) \n",
    "    combine[c] = lbl.transform(list(combine[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape all_data: {}'.format(combine.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (combine.isnull().sum()/len(combine))\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\n",
    "missing_data = pd.DataFrame({'Missing Ratio':all_data_na})\n",
    "missing_data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anything with under 2% missing, just input the mode\n",
    "cols = ('opinion_seas_sick_from_vacc', 'opinion_seas_risk' , 'opinion_seas_vacc_effective',\n",
    "        'opinion_h1n1_vacc_effective','opinion_h1n1_sick_from_vacc', 'opinion_h1n1_risk', 'household_children',\n",
    "        'household_adults', 'behavioral_avoidance', 'behavioral_touch_face', 'h1n1_knowledge', 'h1n1_concern',\n",
    "        'behavioral_outside_home', 'behavioral_large_gatherings', 'behavioral_antiviral_meds', 'behavioral_wash_hands',\n",
    "        'behavioral_face_mask')\n",
    "\n",
    "for c in cols:\n",
    "    combine[c] = combine[c].fillna(combine[c].mode()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (combine.isnull().sum()/len(combine))\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = combine.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['doctor_recc_seasonal', 'doctor_recc_h1n1', 'chronic_med_condition', 'child_under_6_months',\n",
    "                           'health_worker', 'health_insurance']\n",
    "before = []\n",
    "for c in cols:\n",
    "    w = len(combine[c][combine[c]==1])\n",
    "    wo = len(combine[c][combine[c]==0])\n",
    "    pct_w = w / (w + wo)\n",
    "    before.append(pct_w)\n",
    "    print('percentage with', c, pct_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model to fill values\n",
    "# Oversample and plot imbalanced dataset with SMOTE\n",
    "roc = []\n",
    "\n",
    "for index, value in enumerate(cols):\n",
    "    cols = ['health_insurance', 'doctor_recc_seasonal', 'doctor_recc_h1n1', 'chronic_med_condition', 'child_under_6_months',\n",
    "                'health_worker']\n",
    "   \n",
    "    a = cols[index]\n",
    "    cols.pop(index)\n",
    "    X = combine.drop(columns = cols)\n",
    "    W = X[X[a].notnull()]\n",
    "    y = W[a]\n",
    "    X = X.drop([a], axis = 1)\n",
    "    W = W.drop([a], axis = 1)\n",
    "    \n",
    "    oversample = SMOTE()\n",
    "    W, y = oversample.fit_resample(W,y)\n",
    "    \n",
    "    #model = DecisionTreeClassifier()\n",
    "    model = GradientBoostingClassifier()\n",
    "    # evaluate pipeline\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, W, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    print('mean roc', a, mean(scores))\n",
    "    #fit model\n",
    "    model.fit(W,y)\n",
    "    \n",
    "    #make predictions\n",
    "    combine['preds'] = model.predict(X)\n",
    "    combine[a] = np.where(combine[a].isnull(), \n",
    "                                           combine['preds'], combine[a])\n",
    "    combine = combine.drop(['preds'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all NAs are gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create baseline model with logistic regression\n",
    "train = combine[combine['label'] == 1]\n",
    "test = combine[combine['label'] == 0]\n",
    "train = train.drop(['label'], axis = 1)\n",
    "test = test.drop(['label'], axis = 1)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use grid search to find best parameters\n",
    "\n",
    "\"\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# define the model with default hyperparameters\n",
    "model = GradientBoostingClassifier()\n",
    "X, y = train, h1n1\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10,50, 100]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01]\n",
    "grid['subsample'] = [0.5, 0.7, 1.0]\n",
    "grid['max_depth'] = [3, 7, 9]\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    Best: 0.822331 using {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
    "\n",
    "Best: 0.822331 using {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare ensemble to each baseline classifier\n",
    "\n",
    "  \n",
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', LogisticRegression()))\n",
    "    level0.append(('rf', RandomForestClassifier()))\n",
    "    level0.append(('boost', GradientBoostingClassifier()))\n",
    "    level0.append(('et', ExtraTreesClassifier()))\n",
    "    level0.append(('ada', AdaBoostClassifier()))\n",
    "    # define meta learner model\n",
    "    level1 = LogisticRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['rf'] = RandomForestClassifier()\n",
    "    models['boost'] = GradientBoostingClassifier()\n",
    "    models['et'] = ExtraTreesClassifier()\n",
    "    models['ada'] = AdaBoostClassifier()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "#set data\n",
    "X,y = (train, seasonal)\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set data\n",
    "X = train\n",
    "y = seasonal\n",
    "#y = h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter = 1000)))\n",
    "level0.append(('rf', RandomForestClassifier()))\n",
    "level0.append(('boost', GradientBoostingClassifier()))\n",
    "#Best: 0.822331 using {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
    "level0.append(('et', ExtraTreesClassifier()))\n",
    "level0.append(('ada', AdaBoostClassifier()))\n",
    "# define meta learner model\n",
    "level1 = LogisticRegression()\n",
    "# define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "# fit the model on all available data\n",
    "model.fit(X, y)\n",
    "# make a prediction for one example\n",
    "yhat = model.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_h1n1_final = pd.DataFrame(yhat, columns = ['first_class', 'second_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_seasonal_final = pd.DataFrame(yhat, columns = ['first_class', 'second_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.concat([ID, y_pred_h1n1_final['second_class'],y_pred_seasonal_final['second_class']], axis = 1)\n",
    "response.columns = ['respondent_id','h1n1_vaccine', 'seasonal_vaccine']\n",
    "response.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.8408\n",
    "#289 out of 1936. top 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.astype(int)\n",
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutual information\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = train.dtypes == int\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(train, seasonal, discrete_features)\n",
    "mi_scores2 = make_mi_scores(train, h1n1, discrete_features)\n",
    "print(mi_scores[::3])  # show seasonal scores\n",
    "print(mi_scores[::3])  #show h1n1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores - Seasonal Vaccine\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores - H1N1 Vaccine\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mi_scores[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores2[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will keep all columns. There are some bad ones. But common sense tells me some of these columns matter. and the DF is not\n",
    "#all that wide to start with. 35 total columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engineer new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['family_size'] = combine['household_adults'] + combine['household_children']\n",
    "combine['total_behaviors'] = combine['behavioral_antiviral_meds'] + combine['behavioral_avoidance'] + combine['behavioral_face_mask'] + combine['behavioral_wash_hands'] + combine['behavioral_large_gatherings'] + combine['behavioral_outside_home'] + combine['behavioral_touch_face']\n",
    "combine['total_reccs'] = combine['doctor_recc_h1n1'] + combine['doctor_recc_seasonal']\n",
    "combine['pro_vaxx'] =  combine['opinion_h1n1_risk'] + combine['opinion_h1n1_sick_from_vacc'] + combine['opinion_seas_risk'] + combine['opinion_seas_sick_from_vacc']\n",
    "combine['works'] = combine['opinion_h1n1_vacc_effective'] + combine['opinion_seas_vacc_effective']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create baseline model with logistic regression\n",
    "train_new = combine[combine['label'] == 1]\n",
    "test_new = combine[combine['label'] == 0]\n",
    "train_new = train_new.drop(['label'], axis = 1)\n",
    "test_new = test_new.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_new\n",
    "y = seasonal\n",
    "#y = h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_new.shape)\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('lr', LogisticRegression(max_iter = 1000)))\n",
    "level0.append(('rf', RandomForestClassifier()))\n",
    "level0.append(('boost', GradientBoostingClassifier()))\n",
    "#Best: 0.822331 using {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
    "level0.append(('et', ExtraTreesClassifier()))\n",
    "level0.append(('ada', AdaBoostClassifier()))\n",
    "# define meta learner model\n",
    "level1 = LogisticRegression()\n",
    "# define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "# fit the model on all available data\n",
    "model.fit(X, y)\n",
    "# make a prediction for one example\n",
    "yhat = model.predict_proba(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_seasonal_final = pd.DataFrame(yhat, columns = ['first_class', 'second_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_h1n1_final = pd.DataFrame(yhat, columns = ['first_class', 'second_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.concat([ID, y_pred_h1n1_final['second_class'],y_pred_seasonal_final['second_class']], axis = 1)\n",
    "response.columns = ['respondent_id','h1n1_vaccine', 'seasonal_vaccine']\n",
    "response.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
