{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import (mean, std)\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import (cross_val_score, RepeatedStratifiedKFold)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import (StackingClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, \n",
    "                             AdaBoostClassifier)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_set_features.csv')\n",
    "test['label'] = 'test'\n",
    "train = pd.read_csv('training_set_features.csv')\n",
    "train['label'] = 'train'\n",
    "labels = pd.read_csv('training_set_labels.csv')\n",
    "h1n1 = labels['h1n1_vaccine']\n",
    "seasonal = labels['seasonal_vaccine']\n",
    "combine = pd.concat([train, test], axis = 0)\n",
    "ID = test['respondent_id']\n",
    "combine = combine.drop(['respondent_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_cols = combine.columns\n",
    "labels_cols = labels.columns\n",
    "#check column names\n",
    "print(combine_cols)\n",
    "print(labels_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the response variables\n",
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26707 total rows\n",
    "#21.2% got h1n1_vaccine\n",
    "#46.6% got seasonal vaccine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (combine.isnull().sum()/len(combine))\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\n",
    "missing_data = pd.DataFrame({'Missing Ratio':all_data_na})\n",
    "missing_data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like employment occupation and employment industry is code for not in work force where blank.\n",
    "combine['employment_industry'] = combine['employment_industry'].fillna('abcde')\n",
    "combine['employment_occupation'] = combine['employment_occupation'].fillna('fghij')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split columns\n",
    "combine[[\"msa\", \"city\"]] = (  # Create two new features\n",
    "    combine[\"census_msa\"]           # from the Policy feature\n",
    "    .str                         # through the string accessor\n",
    "    .split(\",\", expand=True)     # by splitting on \" \"\n",
    "                                 # and expanding the result into separate columns\n",
    ")\n",
    "combine['city'] = combine['city'].fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine['race_and_sex'] = combine['race'] + \"_\" + combine['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = combine.select_dtypes(include = 'object').columns\n",
    "print(label_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process columns, apply LabelEncoder to categorical features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for c in label_enc:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(combine[c].values)) \n",
    "    combine[c] = lbl.transform(list(combine[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape all_data: {}'.format(combine.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (combine.isnull().sum()/len(combine))\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\n",
    "missing_data = pd.DataFrame({'Missing Ratio':all_data_na})\n",
    "missing_data_reverse = missing_data.sort_values(by = ['Missing Ratio'], ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anything with under 2% missing, just input the mode\n",
    "cols = ('opinion_seas_sick_from_vacc', 'opinion_seas_risk' , 'opinion_seas_vacc_effective',\n",
    "        'opinion_h1n1_vacc_effective','opinion_h1n1_sick_from_vacc', 'opinion_h1n1_risk', 'household_children',\n",
    "        'household_adults', 'behavioral_avoidance', 'behavioral_touch_face', 'h1n1_knowledge', 'h1n1_concern',\n",
    "        'behavioral_outside_home', 'behavioral_large_gatherings', 'behavioral_antiviral_meds', 'behavioral_wash_hands',\n",
    "        'behavioral_face_mask')\n",
    "\n",
    "for c in cols:\n",
    "    combine[c] = combine[c].fillna(combine[c].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (combine.isnull().sum()/len(combine))\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:40]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,12))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)\n",
    "cols = list(all_data_na.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = combine.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = []\n",
    "for c in cols:\n",
    "    w = len(combine[c][combine[c]==1])\n",
    "    wo = len(combine[c][combine[c]==0])\n",
    "    pct_w = w / (w + wo)\n",
    "    before.append(pct_w)\n",
    "    print('percentage with', c, pct_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model to fill values\n",
    "# Oversample and plot imbalanced dataset with SMOTE\n",
    "roc = []\n",
    "\n",
    "for index, value in enumerate(cols):\n",
    "    \n",
    "    a = cols[index] #sets variable to index\n",
    "    cols.pop(index) #removes variable from index for next round\n",
    "    X = combine.drop(columns = cols) #drops all cols that still have NAs to build model\n",
    "    W = X[X[a].notnull()] # creates df with current index but drops all the NAs\n",
    "    y = W[a] #makes the target the index\n",
    "    X = X.drop([a], axis = 1) #drops target from test df\n",
    "    W = W.drop([a], axis = 1) #drops target from training df\n",
    "    \n",
    "    oversample = SMOTE()\n",
    "    W, y = oversample.fit_resample(W,y)\n",
    "    \n",
    "    #model = GradientBoostingClassifier()\n",
    "    model = GradientBoostingClassifier()\n",
    "    # evaluate pipeline\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, W, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "    print('mean roc', a, mean(scores))\n",
    "    #fit model\n",
    "    model.fit(W,y)\n",
    "    \n",
    "    #make predictions\n",
    "    combine['preds'] = model.predict(X)\n",
    "    combine[a] = np.where(combine[a].isnull(), \n",
    "                                           combine['preds'], combine[a])\n",
    "    combine = combine.drop(['preds'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "combine['family_size'] = combine['household_adults'] + combine['household_children']\n",
    "combine['total_behaviors'] = combine['behavioral_antiviral_meds'] + combine['behavioral_avoidance'] + combine['behavioral_face_mask'] + combine['behavioral_wash_hands'] + combine['behavioral_large_gatherings'] + combine['behavioral_outside_home'] + combine['behavioral_touch_face']\n",
    "combine['total_reccs'] = combine['doctor_recc_h1n1'] + combine['doctor_recc_seasonal']\n",
    "combine['pro_vaxx'] =  combine['opinion_h1n1_risk'] + combine['opinion_h1n1_sick_from_vacc'] + combine['opinion_seas_risk'] + combine['opinion_seas_sick_from_vacc']\n",
    "combine['works'] = combine['opinion_h1n1_vacc_effective'] + combine['opinion_seas_vacc_effective']\n",
    "\n",
    "# Create cluster features\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters = 2) \n",
    "combine['Cluster'] = kmeans.fit_predict(combine)\n",
    "combine['Cluster'] = combine['Cluster'].astype(\"category\")\n",
    "demo = combine[['age_group', 'education', 'sex', 'race', 'income_poverty', 'marital_status']]\n",
    "combine['demographics'] = kmeans.fit_predict(demo)\n",
    "combine['demographics'] = combine['demographics'].astype('category')\n",
    "behaviors = combine[['behavioral_antiviral_meds', 'behavioral_avoidance', 'behavioral_face_mask', 'behavioral_wash_hands', 'behavioral_large_gatherings', 'behavioral_outside_home', 'behavioral_touch_face']]\n",
    "combine['behaviors'] = kmeans.fit_predict(behaviors)\n",
    "combine['behaviors'] = combine['behaviors'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = combine.apply(pd.to_numeric)\n",
    "combine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break data back into train and test\n",
    "train_new = combine[combine['label'] == 1]\n",
    "test_new = combine[combine['label'] == 0]\n",
    "train_new = train_new.drop(['label'], axis = 1)\n",
    "test_new = test_new.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutual information\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = train_new.dtypes == int\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(train_new, seasonal, discrete_features)\n",
    "mi_scores2 = make_mi_scores(train_new, h1n1, discrete_features)\n",
    "print(mi_scores[::3])  # show seasonal scores\n",
    "print(mi_scores[::3])  #show h1n1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.rc('ytick', labelsize = 6)\n",
    "    plt.title(\"Mutual Information Scores - Seasonal Vaccine\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.rc('ytick', labelsize = 6)\n",
    "    plt.title(\"Mutual Information Scores - H1N1 Vaccine\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_new.shape)\n",
    "print(test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare ensemble to each baseline classifier\n",
    "  \n",
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('rf', RandomForestClassifier()))\n",
    "    level0.append(('boost', GradientBoostingClassifier()))\n",
    "    level0.append(('et', ExtraTreesClassifier()))\n",
    "    level0.append(('ada', AdaBoostClassifier()))\n",
    "    level0.append(('xgb', XGBClassifier()))\n",
    "    level0.append(('lgbm', lgb.LGBMClassifier()))\n",
    "    # define meta learner model\n",
    "    level1 = LogisticRegression()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "    return model\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['rf'] = RandomForestClassifier()\n",
    "    models['boost'] = GradientBoostingClassifier()\n",
    "    models['et'] = ExtraTreesClassifier()\n",
    "    models['ada'] = AdaBoostClassifier()\n",
    "    models['xgb'] = XGBClassifier()\n",
    "    models['lgbm'] = lgb.LGBMClassifier()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "#set data\n",
    "X,y = (train_new, seasonal)\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_new\n",
    "#y = seasonal\n",
    "y = h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV grid search\n",
    "\n",
    "#GradientBoost Best: 0.822331 using {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5} for h1n1 and seasonal\n",
    "#AdaBoost Best: 0.782005 using {'learning_rate': 1.0, 'n_estimators': 500} for seasonal\n",
    "#AdaBoost Best: 0.833440 using {'learning_rate': 1.0, 'n_estimators': 100} for h1n1\n",
    "#RandomForest Best: 0.778535 using {'max_features': 0.25, 'min_samples_split': 6, 'n_estimators': 250} for seasonal, h1n1\n",
    "#ExtraTreesClassifier Best: 0.776314 using {'criterion': 'gini', 'max_depth': 15} seasonal\n",
    "#ExtraTreesClassifier Best: 0.833465 using {'entropy': 'gini', 'max_depth': 15} h1n1\n",
    "#LogisticRegression Best: 0.774591 using {'C': 0.01, 'penalty': 'l2'} seasonal\n",
    "#LogisticRegression Best: 0.832815 using {'C': 1, 'penalty': 'l2'} h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('rf', RandomForestClassifier()))\n",
    "level0.append(('boost', GradientBoostingClassifier()))\n",
    "level0.append(('et', ExtraTreesClassifier()))\n",
    "level0.append(('ada', AdaBoostClassifier()))\n",
    "level0.append(('xgb', XGBClassifier()))\n",
    "level0.append(('lgb', lgb.LGBMClassifier()))\n",
    "# define meta learner\n",
    "level1 = LogisticRegression()\n",
    "# define the stacking ensemble\n",
    "model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n",
    "# fit the model on all available data\n",
    "model.fit(X, y)\n",
    "# make predictions\n",
    "yhat = model.predict_proba(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_seasonal_final = pd.DataFrame(yhat, columns = ['first_class', 'second_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_h1n1_final = pd.DataFrame(yhat, columns = ['first_class', 'second_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.concat([ID, y_pred_h1n1_final['second_class'],y_pred_seasonal_final['second_class']], axis = 1)\n",
    "response.columns = ['respondent_id','h1n1_vaccine', 'seasonal_vaccine']\n",
    "response.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
